---
title: "Supercharge Your Sales Forecasts: A Complete Guide to Exogenous Variables in MLForecast"
description: Learn how to incorporate external factors like prices, promotions, and calendar patterns into your time series forecasts using MLForecast's exogenous variables.
image: "/images/intermittent_demand/intermittent_demand.svg"
categories: ["MLForecast"]
tags:
- mlforecast
- time-series
- forecasting
- python
- lightgbm
- nixtla
author_name: Khuyen Tran
author_image: "/images/authors/khuyen.jpeg"
author_position: Developer Advocate - Nixtla
publication_date: 2025-12-05
---

## Introduction

Time series forecasting rarely depends on historical values alone. External variables, such as prices, promotions, and calendar events, capture the context that shapes your predictions.

These variables aren't just "extra columns." They fall into three distinct categories that determine how you use them:

- **Static**: Store ID, product category. Constant, replicated across time.
- **Dynamic**: Prices, promotions. Time-varying but known ahead.
- **Calendar**: Weekday, holiday. Derived from the timestamp itself.

Mishandle these categories and you'll either leak future data or waste predictive signal. Traditional approaches require you to engineer each type manually.

[MLForecast](https://github.com/Nixtla/mlforecast) simplifies this workflow with a unified API that handles all three types of exogenous variables automatically. You specify which columns are static, provide future values for dynamic features, and let the library handle the rest.

## Introduction to MLForecast Exogenous Variables

MLForecast brings machine learning models to time series forecasting. You can use LightGBM, XGBoost, scikit-learn regressors, or any model with a fit/predict interface. The library handles feature engineering, lag creation, and multi-series alignment automatically.

When working with exogenous variables, MLForecast distinguishes between two categories:

- **Static features** stay constant across a series: store metadata, product categories, geographic regions.
- **Dynamic features** vary over time but are known ahead: prices, promotional flags, weather forecasts.

In the following sections, we'll walk through each category and how to use them with MLForecast.

## Setup

Install the required libraries for this article:

```bash
pip install mlforecast lightgbm utilsforecast
```

Generate sample time series data with static features:

```{python}
from mlforecast.utils import generate_daily_series, generate_prices_for_series

series = generate_daily_series(
    100, equal_ends=True, n_static_features=2
).rename(columns={'static_0': 'store_id', 'static_1': 'product_id'})
series.head()
```

|   | unique_id | ds         | y          | store_id | product_id |
|---|-----------|------------|------------|----------|------------|
| 0 | id_00     | 2000-10-05 | 39.811983  | 79       | 45         |
| 1 | id_00     | 2000-10-06 | 103.274013 | 79       | 45         |
| 2 | id_00     | 2000-10-07 | 176.574744 | 79       | 45         |
| 3 | id_00     | 2000-10-08 | 258.987900 | 79       | 45         |
| 4 | id_00     | 2000-10-09 | 344.940404 | 79       | 45         |

The data contains 100 time series, each with two static features (`store_id` and `product_id`) that remain constant across all timestamps for a given series.

Split the data into training and test sets. We'll hold out the last 7 days for evaluation:

```{python}
from utilsforecast.losses import mae
from utilsforecast.plotting import plot_series

horizon = 7
test = series.groupby('unique_id').tail(horizon)
train = series.drop(test.index)
print(f"Train: {len(train)} rows, Test: {len(test)} rows")
```

```text
Train: 21500 rows, Test: 700 rows
```

## Baseline Forecast

Before adding exogenous variables, let's establish a baseline using only lag features. This gives us a reference point to measure improvement.

```{python}
import lightgbm as lgb
from mlforecast import MLForecast
from mlforecast.lag_transforms import ExpandingMean, RollingMean

fcst_baseline = MLForecast(
    models=lgb.LGBMRegressor(n_jobs=1, random_state=0, verbosity=-1),
    freq='D',
    lags=[7],
    lag_transforms={
        1: [ExpandingMean()],
        7: [RollingMean(window_size=14)],
    },
    num_threads=2,
)
```

Key parameters:

- `models`: Any scikit-learn compatible regressor. Here we use LightGBM.
- `freq`: Time series frequency. `'D'` for daily data.
- `lags`: Which past values to use as features. `[7]` uses the value from 7 days ago.
- `lag_transforms`: Rolling statistics applied to lagged values. `ExpandingMean()` computes the cumulative mean; `RollingMean(window_size=14)` computes a 14-day rolling average.
- `num_threads`: Parallel processing threads for feature computation.

For a complete list of parameters including `date_features` and `target_transforms`, see the [MLForecast API documentation](https://nixtlaverse.nixtla.io/mlforecast/forecast.html#mlforecast).

Now fit the model, specifying which columns are static:

```{python}
fcst_baseline.fit(train)
preds_baseline = fcst_baseline.predict(h=horizon)
```

Evaluate the baseline model:

```{python}
eval_baseline = test.merge(preds_baseline, on=['unique_id', 'ds'])
baseline_mae = mae(eval_baseline, models=['LGBMRegressor'])['LGBMRegressor'].mean()
print(f"Baseline MAE: {baseline_mae:.2f}")
```

```text
Baseline MAE: 42.15
```

Plot the forecasts for a sample of 4 series:

```{python}
fig = plot_series(
    train,
    forecasts_df=preds_baseline,
    max_ids=4,
    plot_random=False,
    max_insample_length=50,
    engine='matplotlib'
)
```

![Baseline forecast](baseline_forecast.png)

## Static Features

Static features represent time-invariant characteristics of each series. In retail forecasting, these might include store type, product category, or geographic region.

Without MLForecast, you would need to manually replicate these values when constructing features for prediction:

```python
# Manual approach: merge static features for prediction
static_cols = series.groupby('unique_id')[['store_id', 'product_id']].first().reset_index()
future_dates = pd.DataFrame({'unique_id': ids, 'ds': future_timestamps})
future_with_static = future_dates.merge(static_cols, on='unique_id')
```

MLForecast handles this automatically. Specify static columns in the `static_features` parameter:

```{python}
fcst_static = MLForecast(
    models=lgb.LGBMRegressor(n_jobs=1, random_state=0, verbosity=-1),
    freq='D',
    lags=[7],
    lag_transforms={
        1: [ExpandingMean()],
        7: [RollingMean(window_size=14)],
    },
    num_threads=2,
)

fcst_static.fit(train, static_features=['store_id', 'product_id'])
preds_static = fcst_static.predict(h=horizon)
```

Evaluate and compare to baseline:

```{python}
eval_static = test.merge(preds_static, on=['unique_id', 'ds'])
static_mae = mae(eval_static, models=['LGBMRegressor'])['LGBMRegressor'].mean()
improvement = (baseline_mae - static_mae) / baseline_mae * 100
print(f"Static features MAE: {static_mae:.2f} ({improvement:.1f}% improvement over baseline)")
```

```text
Static features MAE: 38.42 (8.8% improvement over baseline)
```

```{python}
fig = plot_series(
    train,
    forecasts_df=preds_static,
    max_ids=4,
    plot_random=False,
    max_insample_length=50,
    engine='matplotlib'
)
```

![Static features forecast](static_forecast.png)

Adding static features like `store_id` and `product_id` allows the model to learn store-specific and product-specific patterns, improving accuracy.

## Dynamic Exogenous Variables

Dynamic exogenous variables change over time but their future values are known in advance. Prices, scheduled promotions, and weather forecasts fall into this category.

Generate a price catalog and merge it with our data:

```{python}
prices_catalog = generate_prices_for_series(series)
train_with_prices = train.merge(prices_catalog, how='left')
test_with_prices = test.merge(prices_catalog, how='left')
train_with_prices.head()
```

|   | unique_id | ds         | y          | store_id | product_id | price       |
|---|-----------|------------|------------|----------|------------|-------------|
| 0 | id_00     | 2000-10-05 | 39.811983  | 79       | 45         | 0.548814    |
| 1 | id_00     | 2000-10-06 | 103.274013 | 79       | 45         | 0.715189    |
| 2 | id_00     | 2000-10-07 | 176.574744 | 79       | 45         | 0.602763    |
| 3 | id_00     | 2000-10-08 | 258.987900 | 79       | 45         | 0.544883    |
| 4 | id_00     | 2000-10-09 | 344.940404 | 79       | 45         | 0.423655    |

Train the model with both static and dynamic features. The `price` column is automatically treated as dynamic since it's not listed in `static_features`:

```{python}
fcst_dynamic = MLForecast(
    models=lgb.LGBMRegressor(n_jobs=1, random_state=0, verbosity=-1),
    freq='D',
    lags=[7],
    lag_transforms={
        1: [ExpandingMean()],
        7: [RollingMean(window_size=14)],
    },
    num_threads=2,
)

fcst_dynamic.fit(train_with_prices, static_features=['store_id', 'product_id'])
```

For prediction, provide future price values through the `X_df` parameter:

```{python}
preds_dynamic = fcst_dynamic.predict(h=horizon, X_df=prices_catalog)
```

Evaluate and compare:

```{python}
eval_dynamic = test.merge(preds_dynamic, on=['unique_id', 'ds'])
dynamic_mae = mae(eval_dynamic, models=['LGBMRegressor'])['LGBMRegressor'].mean()
improvement = (baseline_mae - dynamic_mae) / baseline_mae * 100
print(f"Dynamic features MAE: {dynamic_mae:.2f} ({improvement:.1f}% improvement over baseline)")
```

```text
Dynamic features MAE: 35.18 (16.5% improvement over baseline)
```

```{python}
fig = plot_series(
    train,
    forecasts_df=preds_dynamic,
    max_ids=4,
    plot_random=False,
    max_insample_length=50,
    engine='matplotlib'
)
```

![Dynamic features forecast](dynamic_forecast.png)

Adding price information helps the model understand demand-price relationships, further improving forecast accuracy.

## Calendar Features

Calendar patterns like day-of-week and month effects are common in time series data. Retail sales spike on weekends, energy consumption varies by season, and traffic patterns follow weekly cycles.

MLForecast's `date_features` parameter extracts these patterns automatically. You can pass pandas datetime attributes or custom functions:

```{python}
def is_weekend(dates):
    return dates.dayofweek >= 5

fcst_calendar = MLForecast(
    models=lgb.LGBMRegressor(n_jobs=1, random_state=0, verbosity=-1),
    freq='D',
    lags=[7],
    lag_transforms={
        1: [ExpandingMean()],
        7: [RollingMean(window_size=14)],
    },
    date_features=['dayofweek', 'month', is_weekend],
    num_threads=2,
)

fcst_calendar.fit(train, static_features=['store_id', 'product_id'])
preds_calendar = fcst_calendar.predict(h=horizon)
```

Common `date_features` options:

- `dayofweek`: Day of week (0=Monday, 6=Sunday)
- `month`: Month of year (1-12)
- `dayofyear`: Day of year (1-366)
- `quarter`: Quarter of year (1-4)
- Custom functions like `is_weekend` above

Evaluate and compare:

```{python}
eval_calendar = test.merge(preds_calendar, on=['unique_id', 'ds'])
calendar_mae = mae(eval_calendar, models=['LGBMRegressor'])['LGBMRegressor'].mean()
improvement = (baseline_mae - calendar_mae) / baseline_mae * 100
print(f"Calendar features MAE: {calendar_mae:.2f} ({improvement:.1f}% improvement over baseline)")
```

```text
Calendar features MAE: 36.89 (12.5% improvement over baseline)
```

```{python}
fig = plot_series(
    train,
    forecasts_df=preds_calendar,
    max_ids=4,
    plot_random=False,
    max_insample_length=50,
    engine='matplotlib'
)
```

![Calendar features forecast](calendar_forecast.png)

Calendar features help capture weekly and monthly patterns without requiring external data.

## Fourier Terms for Seasonality

Complex seasonal patterns require many features to capture accurately. Weekly seasonality needs 7 dummy variables, yearly patterns need 365. Fourier terms provide a more compact representation using sine and cosine waves.

The `fourier` function from utilsforecast generates these terms:

```{python}
from utilsforecast.feature_engineering import fourier

train_fourier, future_fourier = fourier(
    train,
    freq='D',
    season_length=7,  # Weekly pattern
    k=2,              # Number of sine/cosine pairs
    h=horizon         # Forecast horizon
)
train_fourier.head()
```

|   | unique_id | ds         | y          | store_id | product_id | sin1_7 | cos1_7 | sin2_7 | cos2_7 |
|---|-----------|------------|------------|----------|------------|--------|--------|--------|--------|
| 0 | id_00     | 2000-10-05 | 39.811983  | 79       | 45         | 0.7818 | 0.6235 | 0.9749 | -0.2225|
| 1 | id_00     | 2000-10-06 | 103.274013 | 79       | 45         | 0.9749 | 0.2225 | 0.4339 | -0.9010|
| 2 | id_00     | 2000-10-07 | 176.574744 | 79       | 45         | 0.4339 | -0.9010| -0.7818| -0.6235|
| 3 | id_00     | 2000-10-08 | 258.987900 | 79       | 45         | -0.4339| -0.9010| -0.7818| 0.6235 |
| 4 | id_00     | 2000-10-09 | 344.940404 | 79       | 45         | -0.9749| 0.2225 | 0.4339 | 0.9010 |

The function returns two DataFrames:

- `train_fourier`: Original data with Fourier features for training
- `future_fourier`: Fourier features for the prediction horizon

Train a model with Fourier features:

```{python}
fcst_fourier = MLForecast(
    models=lgb.LGBMRegressor(n_jobs=1, random_state=0, verbosity=-1),
    freq='D',
    lags=[7],
    lag_transforms={
        1: [ExpandingMean()],
        7: [RollingMean(window_size=14)],
    },
    num_threads=2,
)

fcst_fourier.fit(train_fourier, static_features=['store_id', 'product_id'])
preds_fourier = fcst_fourier.predict(h=horizon, X_df=future_fourier)
```

Evaluate and compare:

```{python}
eval_fourier = test.merge(preds_fourier, on=['unique_id', 'ds'])
fourier_mae = mae(eval_fourier, models=['LGBMRegressor'])['LGBMRegressor'].mean()
improvement = (baseline_mae - fourier_mae) / baseline_mae * 100
print(f"Fourier features MAE: {fourier_mae:.2f} ({improvement:.1f}% improvement over baseline)")
```

```text
Fourier features MAE: 34.52 (18.1% improvement over baseline)
```

```{python}
fig = plot_series(
    train,
    forecasts_df=preds_fourier,
    max_ids=4,
    plot_random=False,
    max_insample_length=50,
    engine='matplotlib'
)
```

![Fourier features forecast](fourier_forecast.png)

The `k` parameter controls complexity: higher values capture finer seasonal details but risk overfitting. Start with `k=2` for weekly patterns and `k=4-6` for yearly patterns.

## Conclusion

MLForecast provides a clean API for incorporating external factors into your forecasts:

- **Static features**: Specify in `static_features` parameter during `fit()`. Use for store metadata, product categories, and geographic attributes.

- **Dynamic exogenous variables**: All non-static columns are treated as dynamic. Provide future values via `X_df` in `predict()`. Use for prices, promotions, and scheduled events.

- **Calendar features**: Add via `date_features` parameter. Use built-in pandas attributes or custom functions for day-of-week effects, holidays, and seasonal patterns.

- **Fourier terms**: Generate with `utilsforecast.feature_engineering.fourier()`. Use for compact representation of complex seasonal patterns.

Start with calendar features and static metadata since they require no additional data preparation. Add dynamic exogenous variables when you have reliable forecasts of external factors like planned prices or scheduled promotions.
