{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbbc887-448e-4651-a5f6-9d38bbbd3119",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \"Forecasting with Privacy at Scale (Nixtla & YData) \"\n",
    "> \"Make your time series data private and then forecast them with Deep Learning models\"\n",
    "\n",
    "- toc: true\n",
    "- branch: main\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Federico Garza\n",
    "- categories: [machine learning, forecasting]\n",
    "- image: images/nixtla_logo.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea14aa-c9be-4111-a6a1-8b7f06c732d3",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99e6a4-8c9d-4da1-84cb-26dc55f1caaa",
   "metadata": {},
   "source": [
    " In this post, we explain how to use [nixtlats](https://github.com/Nixtla/nixtlats) and [ydata-synthetic](https://github.com/ydataai/ydata-synthetic), python libraries to address the problem of data privacy in the context of time series forecasting. We develop a deep learning forecasting pipeline without direct access to the original data and show that data anonymization has a minimal impact on the performance of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91318d40-de5d-4f83-b0f8-308ba228c88e",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1cb5e5-a253-4c48-a803-0fbb8aaf0fb7",
   "metadata": {},
   "source": [
    "In the last decade, neural network-based forecasting methods have become ubiquitous in large-scale forecasting applications, transcending industry boundaries into academia, as it has redefined the state-of-the-art in many practical tasks like demand planning, electricity load forecasting, reverse logistics, weather forecasting, as well as forecasting competitions like the M4 and M5.\n",
    "\n",
    "However, one of the problems for those interested in creating forecasts is data privacy. In many applications, the user does not want the model to have access to the actual data, in particular, if the model training is done in the cloud or outside one's infrastructure. The above dramatically limit the practice, preventing the scaling of models for large datasets using available clouds.\n",
    "\n",
    "This post shows how to solve this problem using `nixtlats` and `ydata-synthetic`.  First, the user can anonymize the data using `ydata-synthetic` and subsequently train state-of-the-art neural forecasting algorithms using `nixtlats` without accessing the original data. Once the model is trained, the model can be sent to the owner of the original data and perform inference in the security of their infrastructure.\n",
    "\n",
    "We evaluate and show the performance of the private model's predictions remains constant compared with the original model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94de2a-4d17-4e66-aef8-f4d5bc4ba676",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50ec74-13d6-4ff5-90a7-777b8e987a2b",
   "metadata": {},
   "source": [
    "The libraries `nixtlats` and `ydata-synthetic` are available in [PyPI](https://pypi.org/project/nixtlats/), so you can install them using `pip install nixtlats` and `pip install ydata-synthetic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9750c59b-f3cb-4925-a11c-7de832e83b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 20:32:41.985003: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-04 20:32:41.985029: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#%gist gistname: libraries-nixtla-ydata.py\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch as t\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "#Nixtla libraries\n",
    "from nixtlats.data.datasets.m4 import M4, M4Info, M4Evaluation\n",
    "from nixtlats.data.tsdataset import TimeSeriesDataset\n",
    "from nixtlats.data.tsloader import TimeSeriesLoader\n",
    "from nixtlats.models.esrnn.esrnn import ESRNN\n",
    "\n",
    "# YData libraries\n",
    "from ydata_synthetic.synthesizers import ModelParameters\n",
    "from ydata_synthetic.synthesizers.timeseries import TimeGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f54910-9a3d-4feb-8b1b-8e1c9a46cbb3",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bd003-a573-442c-b94d-2f65c6e80368",
   "metadata": {},
   "source": [
    "We will show how to anonymize data, forecast it using state-of-the-art deep learning models and generate useful non-anonymized forecasts using M4 competition data. This time series competition has been one of the most important in the world. In particular, we will use the Yearly data and show how even training models with anonymized data yields a similar state-of-the-art performance. The `nixtlats` library provides useful functions to download this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ca35ca-8846-467d-8c7a-2b36e78edbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%gist gistname: data-yearly-nixtla-ydata.py\n",
    "group = M4Info['Yearly']\n",
    "Y_df, _, S_df = M4.load(directory='data', group=group.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046268c4-8022-4d39-a658-22810cc0ae36",
   "metadata": {},
   "source": [
    "In this example, we will use 100 Yearly time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f5d504-21fc-4205-8d67-48a0ae44c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: data-subset-nixtla-ydata.py\n",
    "uids = Y_df['unique_id'].unique()[:1_000]\n",
    "Y_df = Y_df.query('unique_id in @uids')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88febbed-4bd4-4f95-9f7f-10ed81cccfa4",
   "metadata": {},
   "source": [
    "The `M4.load` method returns train and test sets, so we need to split them. The library also provides a wide variety of datasets, [see the documentation](https://nixtla.github.io/nixtlats). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06cd09a4-06ef-4cbe-ab7e-8a7f79e53632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: data-remove-test-nixtla-ydata.py\n",
    "Y_df_test = Y_df.groupby('unique_id').tail(group.horizon).copy()\n",
    "Y_df_train = Y_df.drop(Y_df_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60815e20-7956-4b5c-b24c-f324167e6151",
   "metadata": {},
   "source": [
    "To avoid leakage, we set test values as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bace9a6b-3815-4565-9b3b-93442a957b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: data-test-zerp-nixtla-ydata.py\n",
    "Y_df_test.loc[:, 'y'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e75ebd-0319-446e-a884-76fb620676bc",
   "metadata": {},
   "source": [
    "`nixtlats` requires a dummy test set to make forecasts, so we combine the training data with the testing data with zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce56c330-a772-4ee0-9a55-4fc5ddb3c7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y1</td>\n",
       "      <td>1</td>\n",
       "      <td>5172.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y1</td>\n",
       "      <td>2</td>\n",
       "      <td>5133.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y1</td>\n",
       "      <td>3</td>\n",
       "      <td>5186.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y1</td>\n",
       "      <td>4</td>\n",
       "      <td>5084.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y1</td>\n",
       "      <td>5</td>\n",
       "      <td>5182.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id ds       y\n",
       "0        Y1  1  5172.1\n",
       "1        Y1  2  5133.5\n",
       "2        Y1  3  5186.9\n",
       "3        Y1  4  5084.6\n",
       "4        Y1  5  5182.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%gist gistname: data-full-nixtla-ydata.py upload: both\n",
    "Y_df_full = pd.concat([Y_df_train, Y_df_test]).sort_values(['unique_id', 'ds'], ignore_index=True)\n",
    "Y_df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874ff72b-cd29-41b7-8a0d-6fddc98d572e",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf5175-18fd-427b-9619-b620b7cc905e",
   "metadata": {},
   "source": [
    "### Creating private data using ydata-synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e95fc-1987-4880-a136-ec4d137cb8ba",
   "metadata": {},
   "source": [
    "In this section we make private the training data defined by `Y_df_train` using the `TimeGAN` model from `ydata-synthetic`. You can learn more about the `TimeGAN` model seeing the post [Synthetic Time-Series Data: A GAN approach](https://towardsdatascience.com/synthetic-time-series-data-a-gan-approach-869a984f2239)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb7858c7-46b0-4e7e-b5f6-e1159423d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: pipe-dataset-nixtla-ydata.py\n",
    "train_ts_dataset = TimeSeriesDataset(Y_df=Y_df_train,\n",
    "                                     input_size=4,\n",
    "                                     output_size=group.horizon)\n",
    "y_np = train_ts_dataset.ts_tensor[:,0].cpu().numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f699057-b14f-48c1-845c-3029613183db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: pipe-gan-args-nixtla-ydata.py\n",
    "seq_len, n_seq = y_np.shape\n",
    "hidden_dim = 24\n",
    "gamma = 1\n",
    "\n",
    "noise_dim = 32\n",
    "dim = 128\n",
    "batch_size = 1\n",
    "\n",
    "log_step = 100\n",
    "learning_rate = 5e-4\n",
    "\n",
    "gan_args = ModelParameters(batch_size=batch_size,\n",
    "                           lr=learning_rate,\n",
    "                           noise_dim=noise_dim,\n",
    "                           layers_dim=dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e265fe-3e41-46eb-94b6-a7d00cb03156",
   "metadata": {},
   "source": [
    "The following lines train the `TimeGAN` model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed507e6f-e654-444f-b2f0-6d304b93ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 20:32:43.116115: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-12-04 20:32:43.116348: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-04 20:32:43.116364: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-04 20:32:43.116389: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-89-104): /proc/driver/nvidia/version does not exist\n",
      "2021-12-04 20:32:43.121435: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-04 20:32:43.125442: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "Emddeding network training:   0%|                                                                                                                                                           | 0/50 [00:00<?, ?it/s]2021-12-04 20:32:50.991402: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-12-04 20:32:51.029561: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2999995000 Hz\n",
      "Emddeding network training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:11<00:00,  4.20it/s]\n",
      "Supervised network training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:07<00:00,  7.05it/s]\n",
      "Joint networks training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [02:58<00:00,  3.57s/it]\n"
     ]
    }
   ],
   "source": [
    "#%gist gistname: train-gan-nixtla-ydata.py\n",
    "synth = TimeGAN(model_parameters=gan_args, hidden_dim=24, seq_len=seq_len, n_seq=n_seq, gamma=1)\n",
    "synth.train([y_np], train_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc2779d9-d29e-4dc8-8c75-91940264bdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Synthetic data generation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "#%gist gistname: get-gan-sample-nixtla-ydata.py\n",
    "synth_data = synth.sample(1)[0][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090b8002-08d7-445e-960e-37b1727f4468",
   "metadata": {},
   "source": [
    "Thus, the object `synth_data` contains the training data but now it is anonymized. To use `nixtlats` we need to transform `synth_data` to a pandas dataframe. This can be easy done using the following lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e0eb6b3-bc3b-448d-8c10-70669054a25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.855188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.855188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.855188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.855188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.855188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id ds         y\n",
       "0        Y1  0  0.855188\n",
       "1        Y1  1  0.855188\n",
       "2        Y1  2  0.855188\n",
       "3        Y1  3  0.855188\n",
       "4        Y1  4  0.855188"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%gist gistname: get-synth-training-nixtla-ydata.py upload: both\n",
    "Y_df_train_synth = pd.DataFrame(synth_data.T, index=uids).rename_axis('unique_id')\n",
    "Y_df_train_synth = Y_df_train_synth.stack().rename_axis(['unique_id', 'ds']).rename('y').reset_index()\n",
    "Y_df_train_synth['ds'] = Y_df_train_synth['ds'].astype('object') \n",
    "\n",
    "Y_df_train_synth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3401df-2e0e-44f4-ac95-a189ecd7ae2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Deep Learning model using nixtlats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1857aefb-b3ce-4946-8149-50bd94e0496b",
   "metadata": {},
   "source": [
    "In this section, we use the previous anonymized dataset to train the ESRNN model, the winner of the M4 competition. This model is hybrid; by one hand, it fits each time series locally through an Exponential Smoothing model and then trains the levels using a Recurrent Neural Network. You can learn more about this model by seeing the post [Forecasting in Python with the ESRNN model](https://medium.com/analytics-vidhya/forecasting-in-python-with-esrnn-model-75f7fae1d242)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169987d9-8494-42f5-be3d-63e22c7e5d09",
   "metadata": {},
   "source": [
    "The pipeline for model training follows the logic of deep learning practices. In the first instance a `Dataset` must be instantiated. The `TimeSeriesDataset` class allows to return the complete series in each iteration, this is useful for recurrent models such as ESRNN. To be instantiated, the class receives the target series `Y_df` as a pandas dataframe with columns `unique_id`, `ds` and `y`. Additionally, temporary exogenous variables `X_df` and static variables `S_df` can be included. In this case we only use static variables as in the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d672ff26-177f-4ee6-88a1-cbf1beb880dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: synth-dataset-nixtla-ydata.py\n",
    "train_ts_dataset_synth = TimeSeriesDataset(Y_df=Y_df_train_synth, S_df=S_df,\n",
    "                                           input_size=4,\n",
    "                                           output_size=group.horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4adbb43-dfef-4e99-a086-df38c4ea12be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: synth-loader-nixtla-ydata.py\n",
    "train_ts_loader_synth = TimeSeriesLoader(dataset=train_ts_dataset_synth,\n",
    "                                         batch_size=16,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441da9e-926d-4c2a-9f2e-5d1cc9134e64",
   "metadata": {},
   "source": [
    "The next we need to do is define the ESRNN model included in `nixtlats` as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aa2296d-e840-4727-b842-15f536446ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: synth-model-nixtla-ydata.py\n",
    "model_synth = ESRNN(n_series=group.n_ts,\n",
    "                    n_x=0, n_s=1,\n",
    "                    sample_freq=1,\n",
    "                    input_size=4,\n",
    "                    output_size=group.horizon,\n",
    "                    learning_rate=0.0025,\n",
    "                    lr_scheduler_step_size=6,\n",
    "                    lr_decay=0.08,\n",
    "                    per_series_lr_multip=0.8,\n",
    "                    gradient_clipping_threshold=20,\n",
    "                    rnn_weight_decay=0,\n",
    "                    level_variability_penalty=50,\n",
    "                    testing_percentile=50,\n",
    "                    training_percentile=50,\n",
    "                    cell_type='GRU',\n",
    "                    state_hsize=30,\n",
    "                    dilations=[[1, 2], [2, 6]],\n",
    "                    add_nl_layer=True,\n",
    "                    loss='SMYL',\n",
    "                    val_loss='SMAPE',\n",
    "                    seasonality=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231722c-f2ae-4a8b-b5de-44b3f60df4dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "And then we can train it as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a502291c-919c-446a-8c84-23846164548c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/nixtla-ydata/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=10)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "#%gist gistname: trainer-nixtla-ydata.py\n",
    "trainer = pl.Trainer(max_epochs=15,\n",
    "                     progress_bar_refresh_rate=10, \n",
    "                     deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0d686bf-79de-4674-87ed-420110965de8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/nixtla-ydata/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:118: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\"You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\")\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | _ESRNN | 44.2 K\n",
      "---------------------------------\n",
      "44.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "44.2 K    Total params\n",
      "0.177     Total estimated model params size (MB)\n",
      "/home/ubuntu/miniconda3/envs/nixtla-ydata/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:112: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686f4a44b56a41b58bbec6d88428ed5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%gist gistname: fit-synth-model-nixtla-ydata.py\n",
    "trainer.fit(model_synth, train_ts_loader_synth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbfcc51-f44b-4a51-b12f-9ddfdc1b9b3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model trained with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20764c48-ff97-4bfa-8027-3d41b73fe997",
   "metadata": {},
   "source": [
    "To compare both solutions offer similar results, in this section we train the model with the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08548d94-fbef-4a14-8f49-68a7e1a4197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: real-dataset-nixtla-ydata.py\n",
    "train_ts_dataset = TimeSeriesDataset(Y_df=Y_df_train, S_df=S_df,\n",
    "                                     input_size=4,\n",
    "                                     output_size=group.horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8afe2b60-3764-4d78-9097-4f08f7b02fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: real-loader-nixtla-ydata.py\n",
    "train_ts_loader = TimeSeriesLoader(dataset=train_ts_dataset,\n",
    "                                   batch_size=16,\n",
    "                                   shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a889deac-9a77-473f-94b4-523a0791eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: real-model-nixtla-ydata.py\n",
    "model = ESRNN(n_series=group.n_ts,\n",
    "              n_x=0, n_s=1,\n",
    "              sample_freq=1,\n",
    "              input_size=4,\n",
    "              output_size=group.horizon,\n",
    "              learning_rate=0.0025,\n",
    "              lr_scheduler_step_size=6,\n",
    "              lr_decay=0.08,\n",
    "              per_series_lr_multip=0.8,\n",
    "              gradient_clipping_threshold=20,\n",
    "              rnn_weight_decay=0,\n",
    "              level_variability_penalty=50,\n",
    "              testing_percentile=50,\n",
    "              training_percentile=50,\n",
    "              cell_type='GRU',\n",
    "              state_hsize=30,\n",
    "              dilations=[[1, 2], [2, 6]],\n",
    "              add_nl_layer=True,\n",
    "              loss='SMYL',\n",
    "              val_loss='SMAPE',\n",
    "              seasonality=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c33493-deb7-4343-b88e-88bb78d79f4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "And then we can train it as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19f1f89c-b19c-48c8-9a93-dd4d2f17dbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | _ESRNN | 44.2 K\n",
      "---------------------------------\n",
      "44.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "44.2 K    Total params\n",
      "0.177     Total estimated model params size (MB)\n",
      "/home/ubuntu/miniconda3/envs/nixtla-ydata/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /home/ubuntu/fede/blog/_notebooks/lightning_logs/version_18/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bc8b0731aa4e74aafcdfe8e5a79fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%gist gistname: fit-real-model-nixtla-ydata.py\n",
    "trainer.fit(model, train_ts_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd5fce0-ac82-4a76-a85c-92f6c3032011",
   "metadata": {},
   "source": [
    "### Comparing forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76658ffc-2980-43a7-afc8-d0d6f5952cf1",
   "metadata": {},
   "source": [
    "Finally, we use the original data to make forecasts for both models, `model_synth` trained with synthetic data and `model`, trained with the original data. First, we define the test dataset and loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfc8be45-52c1-4823-aaeb-87982c219cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: test-dataset-nixtla-ydata.py\n",
    "test_ts_dataset = TimeSeriesDataset(Y_df=Y_df_full, S_df=S_df,\n",
    "                                    input_size=4,\n",
    "                                    output_size=group.horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ad82491-1447-4b47-b58f-d754529c4d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: test-loader-nixtla-ydata.py\n",
    "test_ts_loader = TimeSeriesLoader(dataset=test_ts_dataset,\n",
    "                                  batch_size=1024,\n",
    "                                  eq_batch_size=False,\n",
    "                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123f8c7-142b-4925-8d8f-9593bc6be22a",
   "metadata": {},
   "source": [
    "The following lines obtains forecasts with the synthetic model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e67fd4b-cb46-4fe0-8adb-6f427a4ac134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/nixtla-ydata/lib/python3.7/site-packages/nixtlats/data/tsloader.py:43: UserWarning: This class wraps the pytorch `DataLoader` with a special collate function. If you want to use yours simply use `DataLoader`. Removing collate_fn\n",
      "  'This class wraps the pytorch `DataLoader` with a '\n",
      "/home/ubuntu/miniconda3/envs/nixtla-ydata/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:112: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da61162115f44ee7ae1203a908e67de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 63it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%gist gistname: synth-forecasts-nixtla-ydata.py\n",
    "outputs_model_synth = trainer.predict(model_synth, test_ts_loader)\n",
    "_, y_hat_model_synth, _ = zip(*outputs_model_synth)\n",
    "y_hat_model_synth = t.cat([y_hat_[:, -1] for y_hat_ in y_hat_model_synth]).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a13696e-9438-4a90-bac7-38ea4ecbe6b7",
   "metadata": {},
   "source": [
    "Likewise, the following lines obtaines forecasts with the model trained with real data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76f0e3dd-7ddb-4640-8036-1a25e60f856e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880f6def8b684c20a1f1d170218ee295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 63it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%gist gistname: real-forecasts-nixtla-ydata.py\n",
    "outputs_model = trainer.predict(model, test_ts_loader)\n",
    "_, y_hat_model, _ = zip(*outputs_model)\n",
    "y_hat_model = t.cat([y_hat_[:, -1] for y_hat_ in y_hat_model]).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1deb2-d163-4e09-b7bd-52d34308e8e9",
   "metadata": {},
   "source": [
    "Now we compare the performance of both models against the real value using the Mean Average Percentage Error (MAPE) and its symmetric version (SMAPE). `nixtlats` provides functions to easily do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ed648cc-ff64-4108-99c3-78d200251d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%gist gistname: test-set-eval-nixtla-ydata.py\n",
    "Y_df_test, _, S_df = M4.load(directory='data', group=group.name)\n",
    "Y_df_test = Y_df.groupby('unique_id').tail(group.horizon).copy()\n",
    "Y_df_test = Y_df_test.query('unique_id in @uids')\n",
    "Y_df_test['ds'] = Y_df_test.groupby('unique_id')['ds'].transform(lambda x: np.arange(len(x)))\n",
    "y_test = Y_df_test.set_index(['unique_id', 'ds']).unstack().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "031681d2-476f-4670-be39-47db699e01d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAPE</th>\n",
       "      <th>SMAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <td>27.939907</td>\n",
       "      <td>19.686992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Synthetic</th>\n",
       "      <td>25.937980</td>\n",
       "      <td>19.924676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                MAPE      SMAPE\n",
       "Real       27.939907  19.686992\n",
       "Synthetic  25.937980  19.924676"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%gist gistname: evaluation-nixtla-ydata.py upload: both\n",
    "from nixtlats.losses.numpy import mape, smape\n",
    "\n",
    "metrics_dict = {'MAPE': [mape(y_test, y_hat_model),\n",
    "                         mape(y_test, y_hat_model_synth)],\n",
    "                'SMAPE': [smape(y_test, y_hat_model),\n",
    "                          smape(y_test, y_hat_model_synth)]}\n",
    "\n",
    "results = pd.DataFrame(metrics_dict, index=['Real', 'Synthetic'])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d8191b-0d37-4273-881c-b9ce20287af2",
   "metadata": {},
   "source": [
    "As can we see, even the model trained with synthetic data generated with `ydata-synthetic` produces better forecasts considering the MAPE loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a71c34-1d8f-43d9-84a3-0f55c04f7738",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5287c1-2cac-41a3-83ff-324a61e18a13",
   "metadata": {},
   "source": [
    "Data privacy is a common concern for many large companies. Here we will show a complete pipeline for anonymizing data and using it to train state-of-the-art Deep Learning models. As we saw, performance is not harmed, and even for some metrics, it is even better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla-ydata",
   "language": "python",
   "name": "nixtla-ydata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
