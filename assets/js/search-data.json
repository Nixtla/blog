{
  
    
        "post0": {
            "title": "Forecasting with Statistical Models",
            "content": "TL;DR . In this post we present statsforecast, an open-source framework from Nixtla that makes the use of statistical models in forecasting tasks fast and easy. It allows you to focus on the model instead of implementation details. statsforecast is able to handle thousands of time series and is very efficient both time and memory wise. With this library you can easily create benchmarks on which to build more complex models. . Introduction . At Nixtla, we are creating the world&#39;s largest open-source ecosystem for time series forecasting. In this post we will talk about how to use statistical models in forecasting tasks, in particular, we introduce statsforecast, a python library that allows to fit statistical models in a simple and computationally efficient way for thousands of time series, so you can obtain benchmarks results easily. . Motivation . Deep learning and Machine Learning models have demonstrated state-of-the-art performance in time series forecasting tasks. However, it is useful to have a battery of simpler models with which to validate the value added. . In business problems, metrics such as Forecast Value Added (FVA) are usually used to compare the value added of more complex models against techniques that are simpler to implement and explain to decision makers. . A wide range of statistical base models are included in statsforecast that can be used for decision making or as benchmarks for the implementation of more complete models. . Also included are models for specific tasks, such as forecasting sparse (or inttermitent) time series, i.e. time series with a high percentage of zero values, such as sales. These particular models exist in implementations for the R programming language but not for Python. . statsforecast . To make benchmarking easier, we created statsforecast, which is a framework to help you forecast time series using statistical models. You just need to give it a model you want to use and let statsforecast do the rest. . Included models . ADIDA: Temporal aggregation is used for reducing the presence of zero observations, thus mitigating the undesirable effect of the variance observed in the intervals. ADIDA uses equally sized time buckets to perform non-overlapping temporal aggregation and predict the demand over a pre-specified lead-time. The time bucket is set equal to the mean inter-demand interval. SES is used to obtain the forecasts. | Croston Classic: The method proposed by Croston to forecast series that display intermittent demand. The method decomposes the original series into the non-zero demand size and the inter-demand intervals and models them using Simple Exponential Smoothing with a predefined parameter. | Croston SBA: SBA stands for Syntetos-Boylan Approximation. A variant of the Croston&#39;s method that utilizes a debiasing factor. | Croston Optimized: Like Croston, but this model optimizes the Simple Exponential Smoothing for both the non-zero demand size and the inter-demand intervals. | Historic average: Simple average of the time series. | iMAPA: iMAPA stands for Intermittent Multiple Aggregation Prediction Algorithm. Another way for implementing temporal aggregation in demand forecasting. However, in contrast to ADIDA that considers a single aggregation level, iMAPA considers multiple ones, aiming at capturing different dynamics of the data. Thus, iMAPA proceeds by averaging the derived point forecasts, generated using SES. | Naive: Uses the last value of the time series as forecast. The simplest model for time series forecasting. | Random Walk with Drift: Projects the historic trend from the last observed value. | Seasonal Exponential Smoothing: Adjusts a Simple Exponential Smoothing model for each seasonal period. | Seasonal Naive: Like Naive, but this time the forecasts of the model are equal to the last known observation of the same period in order for it to capture possible weekly seasonal variations. | Seasonal Window Average: Uses the last window (defined by the user) to calculate an average for each seasonal period. | SES: SES stands for Simple Exponential Smoothing. This model recursively weights the most recent observations in the time series. Useful for time series with no trend. | TSB: TSB stands for Teunter-Syntetos-Babai. A modification to Croston&#39;s method that replaces the inter-demand intervals component with the demand probability. | Window Average: Uses the last window (defined by the user) to calculate an average. | . Usage . statsforecast is available in PyPI (pip install statsforecast). . Libraries . import random from itertools import product from IPython.display import display, Markdown from multiprocessing import cpu_count import matplotlib.pyplot as plt import pandas as pd from nixtlats.data.datasets.m5 import M5, M5Evaluation from statsforecast import StatsForecast from statsforecast.models import ( adida, croston_classic, croston_sba, croston_optimized, historic_average, imapa, naive, random_walk_with_drift, seasonal_exponential_smoothing, seasonal_naive, seasonal_window_average, ses, tsb, window_average ) . def display_df(df): display(Markdown(df.to_markdown())) . Data . In this example we use the M5 time series competition data. The objective of the competition was to validate models for intermittent demand (sales) data. To download the data we used nixtlats, a library created by the Nixtla team. . series, _, S_df = M5.load(&#39;data&#39;) horizon = 28 . The function M5.load returns train + test data, so we need to separate them. . series_test = series.groupby(&#39;unique_id&#39;).tail(horizon).copy() series = series.drop(series_test.index) . series[&#39;unique_id&#39;] = series[&#39;unique_id&#39;].astype(&#39;object&#39;) series = series.set_index(&#39;unique_id&#39;) display_df(series.head()) . unique_id ds y . FOODS_1_001_CA_1 | 2011-01-29 00:00:00 | 3 | . FOODS_1_001_CA_1 | 2011-01-30 00:00:00 | 0 | . FOODS_1_001_CA_1 | 2011-01-31 00:00:00 | 0 | . FOODS_1_001_CA_1 | 2011-02-01 00:00:00 | 1 | . FOODS_1_001_CA_1 | 2011-02-02 00:00:00 | 4 | . series.index.unique().shape . (30490,) . This is the required input format. . an index named unique_id that identifies each time series. In this example, we have 30,490 time series. | a ds column with the dates. | a y column with the values. | . Training . We now define the statistical models we will use. We must define a list of functions. If the model has additional parameters, besides the forecast horizon, it must be included as a tuple with the model and the additional parameters. . seasonality = 7 #daily data . models = [ adida, croston_classic, croston_sba, croston_optimized, historic_average, imapa, naive, random_walk_with_drift, (seasonal_exponential_smoothing, seasonality, 0.2), (seasonal_naive, seasonality), (seasonal_window_average, seasonality, 2 * seasonality), (ses, 0.1), (tsb, 0.3, 0.2), (window_average, 2 * seasonality) ] . Now we define our trainer, StatsForecast, where we define the models we want to use, the frequency of the data and the number of cores used to parallelize the training job. . In this way adjusting these models and generating forecasts is as simple as the following lines. . fcst = StatsForecast(series, models=models, freq=&#39;D&#39;, n_jobs=cpu_count()) %time forecasts = fcst.forecast(horizon) display_df(forecasts.head()) . INFO:statsforecast.core:Computing forecasts INFO:statsforecast.core:Computed forecasts for adida. INFO:statsforecast.core:Computed forecasts for croston_classic. INFO:statsforecast.core:Computed forecasts for croston_sba. INFO:statsforecast.core:Computed forecasts for croston_optimized. INFO:statsforecast.core:Computed forecasts for historic_average. INFO:statsforecast.core:Computed forecasts for imapa. INFO:statsforecast.core:Computed forecasts for naive. INFO:statsforecast.core:Computed forecasts for random_walk_with_drift. INFO:statsforecast.core:Computed forecasts for seasonal_exponential_smoothing_season_length-7_alpha-0.2. INFO:statsforecast.core:Computed forecasts for seasonal_naive_season_length-7. INFO:statsforecast.core:Computed forecasts for seasonal_window_average_season_length-7_window_size-14. INFO:statsforecast.core:Computed forecasts for ses_alpha-0.1. INFO:statsforecast.core:Computed forecasts for tsb_alpha_d-0.3_alpha_p-0.2. INFO:statsforecast.core:Computed forecasts for window_average_window_size-14. . CPU times: user 3.06 s, sys: 1.11 s, total: 4.17 s Wall time: 1min 3s . unique_id ds adida croston_classic croston_sba croston_optimized historic_average imapa naive random_walk_with_drift seasonal_exponential_smoothing_season_length-7_alpha-0.2 seasonal_naive_season_length-7 seasonal_window_average_season_length-7_window_size-14 ses_alpha-0.1 tsb_alpha_d-0.3_alpha_p-0.2 window_average_window_size-14 . FOODS_1_001_CA_1 | 2016-05-23 00:00:00 | 0.791852 | 0.898247 | 0.853334 | 0.898247 | 0.786193 | 0.705835 | 0 | -0.00154639 | 0.981245 | 0 | 1.35714 | 0.619817 | 0.402382 | 0.642857 | . FOODS_1_001_CA_1 | 2016-05-24 00:00:00 | 0.791852 | 0.898247 | 0.853334 | 0.898247 | 0.786193 | 0.705835 | 0 | -0.00309278 | 0.483033 | 0 | 1 | 0.619817 | 0.402382 | 0.642857 | . FOODS_1_001_CA_1 | 2016-05-25 00:00:00 | 0.791852 | 0.898247 | 0.853334 | 0.898247 | 0.786193 | 0.705835 | 0 | -0.00463918 | 1.09071 | 0 | 0.785714 | 0.619817 | 0.402382 | 0.642857 | . FOODS_1_001_CA_1 | 2016-05-26 00:00:00 | 0.791852 | 0.898247 | 0.853334 | 0.898247 | 0.786193 | 0.705835 | 0 | -0.00618557 | 1.37597 | 1 | 0.357143 | 0.619817 | 0.402382 | 0.642857 | . FOODS_1_001_CA_1 | 2016-05-27 00:00:00 | 0.791852 | 0.898247 | 0.853334 | 0.898247 | 0.786193 | 0.705835 | 0 | -0.00773196 | 0.592922 | 0 | 1.07143 | 0.619817 | 0.402382 | 0.642857 | . Visualization . In this section we present visual examples of the forecasts generated. . forecasts = forecasts.reset_index().merge(series_test, how=&#39;left&#39;, on=[&#39;unique_id&#39;, &#39;ds&#39;]) . models = forecasts.drop(columns=[&#39;unique_id&#39;, &#39;ds&#39;, &#39;y&#39;]).columns.to_list() . def plot_grid_prediction(y, y_hat, models, plot_random=True, unique_ids=None): &quot;&quot;&quot; y: pandas df panel with columns unique_id, ds, y y_hat: pandas df panel with columns unique_id, ds, y_hat models: List[str] List of models to plot plot_random: bool if unique_ids will be sampled unique_ids: list unique_ids to plot &quot;&quot;&quot; pd.plotting.register_matplotlib_converters() fig, axes = plt.subplots(2, 2, figsize = (24,8)) if not unique_ids: unique_ids = y[&#39;unique_id&#39;].unique() assert len(unique_ids) &gt;= 4, &quot;Must provide at least 4 ts&quot; if plot_random: unique_ids = random.choices(unique_ids, k=4) for i, (idx, idy) in enumerate(product(range(2), range(2))): y_uid = y[y.unique_id == unique_ids[i]] y_uid_hat = y_hat[y_hat.unique_id == unique_ids[i]] axes[idx, idy].plot(y_uid.ds, y_uid.y, label = &#39;y&#39;) for model in models: axes[idx, idy].plot(y_uid_hat.ds, y_uid_hat[model], label=model) axes[idx, idy].set_title(unique_ids[i]) axes[idx, idy].legend(loc=&#39;upper left&#39;) plt.show() . plot_models = random.sample(models, k=4) . plot_grid_prediction(forecasts[[&#39;unique_id&#39;, &#39;ds&#39;, &#39;y&#39;]], forecasts[[&#39;unique_id&#39;, &#39;ds&#39;] + models], models=plot_models) . Evaluation . We now evaluate the performance of the solution. We use the mean of the forecasts as the final forecast. With this library you can easily create benchmarks on which to build more complex models. . forecasts[&#39;y_hat&#39;] = forecasts.drop(columns=[&#39;unique_id&#39;, &#39;ds&#39;, &#39;y&#39;]).mean(axis=1) y_hat = forecasts.set_index([&#39;unique_id&#39;, &#39;ds&#39;])[[&#39;y_hat&#39;]].unstack() y_hat = y_hat.droplevel(0, 1).reset_index() y_hat = y_hat.merge(S_df, how=&#39;left&#39;, on=[&#39;unique_id&#39;]) display_df(M5Evaluation.evaluate(&#39;data&#39;, y_hat)) . /tmp/ipykernel_4639/107357846.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with &#39;numeric_only=None&#39;) is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. forecasts[&#39;y_hat&#39;] = forecasts.drop(columns=[&#39;unique_id&#39;, &#39;ds&#39;, &#39;y&#39;]).mean(axis=1) . wrmsse . Total | 0.905412 | . Level1 | 0.817109 | . Level2 | 0.851129 | . Level3 | 0.870202 | . Level4 | 0.829788 | . Level5 | 0.888269 | . Level6 | 0.882599 | . Level7 | 0.929427 | . Level8 | 0.892819 | . Level9 | 0.938245 | . Level10 | 1.05243 | . Level11 | 0.989029 | . Level12 | 0.923902 | . Thus, we obtain the following table, . model WRMSSE . statsforecast | 0.905 | . MLP | 0.977 | . RF | 1.010 | . Naive | 1.752 | . For a description of the additional results, see this medium post. In conclusion, with statsforecast we obtained good benchmarks quickly. . WIP . statsforecast is a work in progress. In next releases we plan to include: . Automated backtesting. | Ensembles (such as fforma). | More statistical models with exogenous variables. | . Next steps . If you&#39;re interested you can learn more in the following resources: . GitHub repo: https://github.com/Nixtla/statsforecast | Documentation: https://nixtla.github.io/statsforecast/ | .",
            "url": "https://nixtla.github.io/blog/statistics/forecasting/models/2021/11/23/Intro-statsforecast.html",
            "relUrl": "/statistics/forecasting/models/2021/11/23/Intro-statsforecast.html",
            "date": " • Nov 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Replicating ESRNN results with nixtlats (I): M4 Yearly dataset",
            "content": "Introduction . In this post we introduce nixtlats: a library of state-of-the-art deep learning models for time series forecasting written in pytorch, focused on usability and replicability. In this first post we introduce the Exponential Smoothing with Recurrent Neural Networks (ESRNN) model, winner of the M4 time series competition and show that our implementation achieves similar performance to the original submission. Throughout the post we describe the pipeline for training the models using nixtlats and also a brief description of the datasets included in the library. This work is inspired by an earlier implementation of ESRNN in pytorch. . Install nixtlats . The nixtlats library is available in pip so to install it just use: . !pip install nixtlats . GPU usage . To use GPU in colab: Edit &gt; Notebook settings or Runtime&gt;Change runtime type and select GPU as Hardware accelerator. . Import libraries . import numpy as np import pandas as pd import pytorch_lightning as pl import torch as t from pytorch_lightning import loggers as pl_loggers from pytorch_lightning import seed_everything from nixtlats.data.datasets.m4 import M4, M4Info, M4Evaluation from nixtlats.data.tsdataset import TimeSeriesDataset from nixtlats.data.tsloader import TimeSeriesLoader from nixtlats.models.esrnn.esrnn import ESRNN . seed_everything(117982, workers=True) . Global seed set to 117982 . 117982 . Import M4 Yearly data . The nixtlats library provides functions to download and manipulate M4 data. The M4.load method returns train and test sets. The library also provides a wide variety of datasets, see the documentation. . group = M4Info[&#39;Yearly&#39;] Y_df, _, S_df = M4.load(directory=&#39;data&#39;, group=group.name) . Y_df_test = Y_df.groupby(&#39;unique_id&#39;).tail(group.horizon).copy() . Y_df_train = Y_df.drop(Y_df_test.index) . To avoid leakage, set test values as zero. . Y_df_test.loc[:, &#39;y&#39;] = 0 . nixtlats requires a dummy test set to make forecasts. . Y_df_full = pd.concat([Y_df_train, Y_df_test]).sort_values([&#39;unique_id&#39;, &#39;ds&#39;], ignore_index=True) . Define Time Series Datasets . The pipeline for model training follows the logic of deep learning practices. In the first instance a Dataset must be instantiated. The TimeSeriesDataset class allows to return the complete series in each iteration, this is useful for recurrent models such as ESRNN. To be instantiated, the class receives the target series Y_df as a pandas dataframe with columns unique_id, ds and y. Additionally, temporary exogenous variables X_df and static variables S_df can be included. In this case we only use static variables as in the original model. . train_ts_dataset = TimeSeriesDataset(Y_df=Y_df_train, S_df=S_df, input_size=4, output_size=group.horizon) . test_ts_dataset = TimeSeriesDataset(Y_df=Y_df_full, S_df=S_df, input_size=4, output_size=group.horizon) . Define Time Series Loaders . Once the training and test TimeSeriesDatasets are defined, the loaders must be instantiated through TimeSeriesLoader which is a wrapper of the pytorch&#39;s DataLoader, so it can receive all the original arguments. . train_ts_loader = TimeSeriesLoader(dataset=train_ts_dataset, batch_size=16, shuffle=False) . test_ts_loader = TimeSeriesLoader(dataset=test_ts_dataset, batch_size=1024, eq_batch_size=False, shuffle=False) . Define ESRNN model . The instantiation of the model is carried out as follows. For an overview of all parameters see the documentation. . model = ESRNN(n_series=group.n_ts, n_x=0, n_s=1, sample_freq=1, input_size=4, output_size=group.horizon, learning_rate=0.0025, lr_scheduler_step_size=6, lr_decay=0.08, per_series_lr_multip=0.8, gradient_clipping_threshold=20, rnn_weight_decay=0, level_variability_penalty=50, testing_percentile=50, training_percentile=50, cell_type=&#39;GRU&#39;, state_hsize=30, dilations=[[1, 2], [2, 6]], add_nl_layer=True, loss=&#39;SMYL&#39;, val_loss=&#39;SMAPE&#39;, seasonality=[]) . Train model . The nixtlats library is designed to operate with pytorch-lightning so that all its functionalities can be used. Add gpus = 1 for GPU training using colab. . trainer = pl.Trainer(max_epochs=15, progress_bar_refresh_rate=10, deterministic=True) trainer.fit(model, train_ts_loader) . GPU available: False, used: False TPU available: False, using: 0 TPU cores /home/federicogarza2/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop warnings.warn(*args, **kwargs) | Name | Type | Params 0 | model | _ESRNN | 44.2 K 44.2 K Trainable params 0 Non-trainable params 44.2 K Total params 0.177 Total estimated model params size (MB) Global seed set to 117982 /home/federicogarza2/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. warnings.warn(*args, **kwargs) . Make forecasts . The ESRNN model returns for each test batch three elements: y, the test values y_hat and the mask mask. . outputs = trainer.predict(model, test_ts_loader) . /home/federicogarza2/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. warnings.warn(*args, **kwargs) . _, y_hat, mask = zip(*outputs) . The ESRNN model returns forecasts for all windows each step_size. We need the last window only. . y_hat = t.cat([y_hat_[:, -1] for y_hat_ in y_hat]).cpu().numpy() . Evaluate results . An M4 performance evaluator function is included within nixtlats to facilitate the reproduction of the results. . M4Evaluation.evaluate(&#39;data&#39;, &#39;Yearly&#39;, y_hat) . SMAPE MASE OWA . Yearly 13.348044 | 2.974721 | 0.782632 | . Original results . In addition, the results of the original submissions can be evaluated as follows. . esrnn_url = &#39;https://github.com/Nixtla/m4-forecasts/raw/master/forecasts/submission-118.zip&#39; . M4Evaluation.evaluate(&#39;data&#39;, &#39;Yearly&#39;, esrnn_url) . SMAPE MASE OWA . Yearly 13.175716 | 2.979908 | 0.778012 | . Conclusion . In this post we present nixtlats, a time series forecasting library using deep learning. As can be seen for the case of Yearly in M4 the results are similar to those obtained by the original implementation. These results are encouraging because the ensembles of different models used by the original implementation were not considered. . How to contribute . The full code is publicly available at github. To contribute you can fork this repository and make a PR with your improvements. You can also create issues if you have problems running the model. .",
            "url": "https://nixtla.github.io/blog/deep%20learning/forecasting/m4/2021/06/25/esrnn-i.html",
            "relUrl": "/deep%20learning/forecasting/m4/2021/06/25/esrnn-i.html",
            "date": " • Jun 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Forecasting with Machine Learning models",
            "content": "Introduction . We at Nixlta, are trying to make time series forecasting more accesible to everyone. In this post I&#39;ll talk about using machine learning models in forecasting tasks. I&#39;ll use an example to show what the main challanges are and then I&#39;ll introduce mlforecast, a framework that facilitates using machine learning models in forecasting. mlforecast does feature engineering and takes care of the updates for you, the user only has to provide a regressor that follows the scikit-learn API (implements fit and predict) and specify the features that she wants to use. These features can be lags, lag-based transformations and date features. (For further feature creation or an automated forecasting pipeline check fasttsfeatures and autotimeseries . Motivation . For many years classical methods like ARIMA and ETS dominated the forecasting field. One of the reasons was that most of the use cases involved forecasting low-frequency series with monthly, quarterly or yearly granularity. Furthermore, there weren&#39;t many time-series datasets, so fitting a single model to each one and getting forecasts from them was straightforward. . However, in recent years, the need to forecast bigger datasets higher frequencies has risen. Bigger and higher frequency time series imposes a challenge for classical forecasting methods. Those methods aren&#39;t mean to model many time series together, and their implementation is suboptimal and slow (you have to train many models) and besides, there could be some common or shared patterns between the series that could be learned by modeling them together. . To address this problem, there have been various efforts in proposing different methods that can train a single model on many time series. Some fascinating deep learning architectures have been designed that can accurately forecast many time series like ESRNN, DeepAR, NBEATS among others. (Check nixtlats and Replicating ESRNN results for our WIP) . Traditional machine learning models like gradient boosted trees have been used as well and have shown that they can achieve very good performance as well. However, using these models with lag-based features isn&#39;t very straightforward because you have to update your features in every timestep in order to compute the predictions. Additionally, depending on your forecasting horizon and the lags that you use, at some point you run out of real values of your series to update your features, so you have to do something to fill those gaps. One possible approach is using your predictions as the values for the series and update your features using them. This is exactly what mlforecast does for you. . Example . In the following section I&#39;ll show a very simple example with a single series to highlight the difficulties in using machine learning models in forecasting tasks. This will later motivate te use of mlforecast, a library that makes the whole process easier and faster. . Libraries . import matplotlib.pyplot as plt import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.pipeline import make_pipeline from sklearn.preprocessing import OneHotEncoder . Data . rng = np.random.RandomState(90) serie_length = 7 * 20 dates = pd.date_range(&#39;2000-01-01&#39;, freq=&#39;D&#39;, periods=serie_length, name=&#39;ds&#39;) y = dates.dayofweek + rng.randint(-1, 2, size=dates.size) data = pd.DataFrame({&#39;y&#39;: y.astype(np.float64)}, index=dates) data.plot(marker=&#39;.&#39;, figsize=(16, 6)); . Our data has daily seasonality and as you can see in the creation, it is basically just dayofweek + Uniform({-1, 0, 1}). . Training . Let&#39;s say we want forecasts for the next 14 days, the first step would be deciding which model and features to use, so we&#39;ll create a validation set containing the last 14 days in our data. . valid_horizon = 14 train = data.head(-valid_horizon).copy() y_valid = data.tail(valid_horizon)[&#39;y&#39;] . Now we&#39;ll try to find which lags are the most important to use as features. To do this we&#39;ll compute the autocorrelation of the series values with respect to each lag. . max_lags = 28 autocorrelations = np.empty((max_lags, 2)) autocorrelations[:, 0] = np.arange(max_lags) + 1 for lag in range(max_lags): autocorrelations[lag, 1] = np.corrcoef(y[lag + 1:], y[:-(lag + 1)])[0, 1] fig, ax = plt.subplots(figsize=(10, 6)) ax.bar(autocorrelations[:, 0], autocorrelations[:, 1]) ax.set( xlabel=&#39;lag&#39;, ylabel=&#39;Correlation coefficient&#39;, title=&#39;Autocorrelation by lag&#39;, xticks=[lag + 1 for lag in range(0, max_lags, 2)], ); . We can see that the most important lags are multiples of 7. As a starting point we&#39;ll try lag 7 and lag 14. . train[&#39;lag-7&#39;] = train[&#39;y&#39;].shift(7) train[&#39;lag-14&#39;] = train[&#39;y&#39;].shift(14) . Computing lag values leaves some rows with nulls. . train.isnull().sum() . y 0 lag-7 7 lag-14 14 dtype: int64 . We&#39;ll drop these before training. . train_without_nulls = train.dropna() X_train = train_without_nulls.drop(columns=&#39;y&#39;) y_train = train_without_nulls[&#39;y&#39;] . For simplicity sake, we&#39;ll train a linear regression without intercept. Since the best model would be taking the average for each day of the week, we expect to get coefficients that are close to 0.5. . lr = LinearRegression(fit_intercept=False).fit(X_train, y_train) lr.coef_ . array([0.5136953 , 0.44695162]) . This model is taking $0.51 cdot lag_7 + 0.45 cdot lag_{14}$. . Forecasting . Great. We have our trained model. How can we compute the forecast for the next 14 days? Machine learning models a feature matrix X and output the predicted values y. So we need to create the feature matrix X for the next 14 days and give it to our model. . If we want to get the lag-7 for the next day, following the training set, we can just get the value in the 7th position starting from the end. The lag-7 two days after the end of the training set would be the value in the 6th position starting from the end and so on. Similarly for the lag-14. . next_lags_7 = y_train.tail(7).values next_lags_7 . array([ 4., 6., -1., 2., 3., 2., 4.]) . next_lags_14 = y_train.tail(14).values next_lags_14 . array([ 6., 5., -1., 2., 3., 2., 5., 4., 6., -1., 2., 3., 2., 4.]) . As you may have noticed we can only get 7 of the lag-7 values from our history and we can get all 14 values for the lag-14. With this information we can only forecast the next 7 days, so we&#39;ll only take the first 7 values of the lag-14. . X_valid1 = pd.DataFrame({ &#39;lag-7&#39;: next_lags_7, &#39;lag-14&#39;: next_lags_14[:7], }) X_valid1 . lag-7 lag-14 . 0 4.0 | 6.0 | . 1 6.0 | 5.0 | . 2 -1.0 | -1.0 | . 3 2.0 | 2.0 | . 4 3.0 | 3.0 | . 5 2.0 | 2.0 | . 6 4.0 | 5.0 | . With these features we can compute the forecasts for the next 7 days. . forecasts_7 = lr.predict(X_valid1) forecasts_7 . array([ 4.7364909 , 5.31692988, -0.96064692, 1.92129383, 2.88194075, 1.92129383, 4.28953928]) . These values can be interpreted as the values of our series for the next 7 days following the last training date. In order to compute the forecasts following that date we can use these values as if they were the values of our series and use them as lag-7 for the following periods. . In other words, we can fill the rest of our features matrix with these values and the real values of the lag-14. . X_valid2 = pd.DataFrame({ &#39;lag-7&#39;: forecasts_7, &#39;lag-14&#39;: next_lags_14[-7:], }) X_valid2 . lag-7 lag-14 . 0 4.736491 | 4.0 | . 1 5.316930 | 6.0 | . 2 -0.960647 | -1.0 | . 3 1.921294 | 2.0 | . 4 2.881941 | 3.0 | . 5 1.921294 | 2.0 | . 6 4.289539 | 4.0 | . As you can see we&#39;re still using the real values of the lag-14 and we&#39;ve plugged in our predictions as the values for the lag-7. We can now use these features to predict the remaining 7 days. . forecasts_7_14 = lr.predict(X_valid2) y_pred = np.hstack([forecasts_7, forecasts_7_14]) y_pred . array([ 4.7364909 , 5.31692988, -0.96064692, 1.92129383, 2.88194075, 1.92129383, 4.28953928, 4.22091958, 5.41299159, -0.94043142, 1.88086285, 2.82129427, 1.88086285, 3.99132264]) . And now we have our forecasts for the next 14 days! This wasn&#39;t that painful but it wasn&#39;t pretty or easy either. And we just used lags which are the easiest feature we can have. . What if we had used lag-1? We would have needed to do this predict-update step 14 times! . And what if we had more elaborate features like the rolling mean over some lag? As you can imagine it can get quite messy and is very error prone. . mlforecast . With these problems in mind we created mlforecast, which is a framework to help you forecast time series using machine learning models. It takes care of all these messy details for you. You just need to give it a model and define which features you want to use and let mlforecast do the rest. . mlforecast is available in PyPI (pip install mlforecast) as well as conda-forge (conda install -c conda-forge mlforecast) . The previously described problem can be solved using mlforecast with the following code. . First we have to set up our data in the required format. . train_mlfcst = train.reset_index()[[&#39;ds&#39;, &#39;y&#39;]] train_mlfcst.index = pd.Index(np.repeat(0, train.shape[0]), name=&#39;unique_id&#39;) train_mlfcst.head() . ds y . unique_id . 0 2000-01-01 | 5.0 | . 0 2000-01-02 | 7.0 | . 0 2000-01-03 | -1.0 | . 0 2000-01-04 | 2.0 | . 0 2000-01-05 | 2.0 | . This is the required input format. . an index named unique_id that identifies each time serie. In this case we only have one but you can have as many as you want. | a ds column with the dates. | a y column with the values. | . Now we&#39;ll import the TimeSeries transformer, where we define the features that we want to use. We&#39;ll also import the Forecast class, which will hold our transformer and model and will run the forecasting pipeline for us. . from mlforecast.core import TimeSeries from mlforecast.forecast import Forecast . We initialize our transformer specifying the lags that we want to use. . ts = TimeSeries(lags=[7, 14]) ts . TimeSeries(freq=&lt;Day&gt;, transforms=[&#39;lag-7&#39;, &#39;lag-14&#39;], date_features=[], num_threads=1) . As you can see this transformer will use lag-7 and lag-14 as features. Now we define our model. . model = LinearRegression(fit_intercept=False) . We create a Forecast object with the model and the time series transformer and fit it to our data. . fcst = Forecast(model, ts) fcst.fit(train_mlfcst) . Forecast(model=LinearRegression(fit_intercept=False), ts=TimeSeries(freq=&lt;Day&gt;, transforms=[&#39;lag-7&#39;, &#39;lag-14&#39;], date_features=[], num_threads=1)) . And now we just call predict with the forecast horizon that we want. . y_pred_mlfcst = fcst.predict(14) y_pred_mlfcst . ds y_pred . unique_id . 0 2000-05-06 | 4.736491 | . 0 2000-05-07 | 5.316930 | . 0 2000-05-08 | -0.960647 | . 0 2000-05-09 | 1.921294 | . 0 2000-05-10 | 2.881941 | . 0 2000-05-11 | 1.921294 | . 0 2000-05-12 | 4.289539 | . 0 2000-05-13 | 4.220920 | . 0 2000-05-14 | 5.412992 | . 0 2000-05-15 | -0.940431 | . 0 2000-05-16 | 1.880863 | . 0 2000-05-17 | 2.821294 | . 0 2000-05-18 | 1.880863 | . 0 2000-05-19 | 3.991323 | . This was a lot easier and internally this did the same as we did before. Lets verify real quick. . Check that we got the same predictions: . np.testing.assert_equal(y_pred, y_pred_mlfcst[&#39;y_pred&#39;].values) . Check that we got the same model: . np.testing.assert_equal(lr.coef_, fcst.model.coef_) . Experiments made easier . Having this high level abstraction allows us to focus on defining the best features and model instead of worrying about implementation details. For example, we can try out different lags very easily by writing a simple function that leverages mlforecast: . def evaluate_lags(lags): ts = TimeSeries(lags=lags) model = LinearRegression(fit_intercept=False) fcst = Forecast(model, ts) fcst.fit(train_mlfcst) print(*[f&#39;lag-{lag:&lt;2} coef: {fcst.model.coef_[i]:.2f}&#39; for i, lag in enumerate(lags)], sep=&#39; n&#39;) y_pred = fcst.predict(14) mse = mean_squared_error(y_valid, y_pred[&#39;y_pred&#39;]) print(f&#39;MSE: {mse:.2f}&#39;) . evaluate_lags([7, 14]) . lag-7 coef: 0.51 lag-14 coef: 0.45 MSE: 0.97 . evaluate_lags([7, 14, 21]) . lag-7 coef: 0.39 lag-14 coef: 0.31 lag-21 coef: 0.29 MSE: 0.74 . evaluate_lags([7, 14, 21, 28]) . lag-7 coef: 0.30 lag-14 coef: 0.18 lag-21 coef: 0.10 lag-28 coef: 0.41 MSE: 0.40 . Backtesting . In the previous examples we manually split our data. The Forecast object also has a backtest method that can do that for us. . We&#39;ll first get all of our data into the required format. . data_mlfcst = data.reset_index() data_mlfcst.index = pd.Index(np.repeat(0, data_mlfcst.shape[0]), name=&#39;unique_id&#39;) data_mlfcst . ds y . unique_id . 0 2000-01-01 | 5.0 | . 0 2000-01-02 | 7.0 | . 0 2000-01-03 | -1.0 | . 0 2000-01-04 | 2.0 | . 0 2000-01-05 | 2.0 | . ... ... | ... | . 0 2000-05-15 | 0.0 | . 0 2000-05-16 | 0.0 | . 0 2000-05-17 | 3.0 | . 0 2000-05-18 | 4.0 | . 0 2000-05-19 | 4.0 | . 140 rows × 2 columns . Now we instantiate a Forecast object as we did previously and call the backtest method instead. . backtest_fcst = Forecast( LinearRegression(fit_intercept=False), TimeSeries(lags=[7, 14]) ) backtest_results = backtest_fcst.backtest(data_mlfcst, n_windows=2, window_size=14) . This returns a generator with the results for each window. . type(backtest_results) . generator . result1 = next(backtest_results) result1 . ds y y_pred . unique_id . 0 2000-04-22 | 6.0 | 4.324606 | . 0 2000-04-23 | 5.0 | 6.312634 | . 0 2000-04-24 | -1.0 | -0.967022 | . 0 2000-04-25 | 2.0 | 0.510503 | . 0 2000-04-26 | 3.0 | 1.477525 | . 0 2000-04-27 | 2.0 | 3.868087 | . 0 2000-04-28 | 5.0 | 4.378590 | . 0 2000-04-29 | 4.0 | 4.033799 | . 0 2000-04-30 | 6.0 | 6.418250 | . 0 2000-05-01 | -1.0 | -0.950186 | . 0 2000-05-02 | 2.0 | 0.717132 | . 0 2000-05-03 | 3.0 | 1.667318 | . 0 2000-05-04 | 2.0 | 3.800745 | . 0 2000-05-05 | 4.0 | 4.517877 | . result2 = next(backtest_results) result2 . ds y y_pred . unique_id . 0 2000-05-06 | 4.0 | 4.736491 | . 0 2000-05-07 | 6.0 | 5.316930 | . 0 2000-05-08 | -1.0 | -0.960647 | . 0 2000-05-09 | 1.0 | 1.921294 | . 0 2000-05-10 | 2.0 | 2.881941 | . 0 2000-05-11 | 3.0 | 1.921294 | . 0 2000-05-12 | 5.0 | 4.289539 | . 0 2000-05-13 | 4.0 | 4.220920 | . 0 2000-05-14 | 6.0 | 5.412992 | . 0 2000-05-15 | 0.0 | -0.940431 | . 0 2000-05-16 | 0.0 | 1.880863 | . 0 2000-05-17 | 3.0 | 2.821294 | . 0 2000-05-18 | 4.0 | 1.880863 | . 0 2000-05-19 | 4.0 | 3.991323 | . result2 here is the same as the evaluation we did manually. . np.testing.assert_equal(result2[&#39;y_pred&#39;].values, y_pred_mlfcst[&#39;y_pred&#39;].values) . We can define a validation scheme for different lags using several windows. . def backtest(ts, model=None): if model is None: model = LinearRegression(fit_intercept=False) fcst = Forecast(model, ts) backtest_results = fcst.backtest(data_mlfcst, n_windows=4, window_size=14) mses = [] results = [] for i, result in enumerate(backtest_results): mses.append(mean_squared_error(result[&#39;y&#39;], result[&#39;y_pred&#39;])) results.append(result.rename(columns={&#39;y_pred&#39;: f&#39;split-{i}&#39;})) pd.concat(results).set_index(&#39;ds&#39;).plot(marker=&#39;.&#39;, figsize=(16, 6)) print(&#39;Splits MSE:&#39;, np.round(mses, 2)) print(f&#39;Mean MSE: {np.mean(mses):.2f}&#39;) . backtest(TimeSeries(lags=[7, 14])) . Splits MSE: [1.36 1.52 1.43 0.97] Mean MSE: 1.32 . backtest(TimeSeries(lags=[7, 14, 21])) . Splits MSE: [1.29 1.23 1.25 0.74] Mean MSE: 1.13 . backtest(TimeSeries(lags=[7, 14, 21, 28])) . Splits MSE: [0.97 1.1 1.13 0.4 ] Mean MSE: 0.90 . Lag transformations . We can specify transformations on the lags as well as just lags. The window_ops library has some implementations of different window functions. You can also define your own transformations. . Let&#39;s try a seasonal rolling mean, this takes the average over the last n seasons, in this case it would be the average of the last n mondays, tuesdays, etc. Computing the updates for this feature would probably be a bit annoying, however using this framework we can just pass it to lag_transforms. If the transformations takes additional arguments (additional to the values of the serie) we specify a tuple like (transform_function, arg1, arg2), which in this case are season_length and window_size. . from window_ops.rolling import seasonal_rolling_mean . help(seasonal_rolling_mean) . Help on CPUDispatcher in module window_ops.rolling: seasonal_rolling_mean(input_array: numpy.ndarray, season_length: int, window_size: int, min_samples: Optional[int] = None) -&gt; numpy.ndarray Compute the seasonal_rolling_mean over the last non-na window_size samples of the input array starting at min_samples. . lag_transforms takes a dictionary where the keys are the lags that we want to apply the transformations to and the values are the transformations themselves. . ts = TimeSeries( lag_transforms={ 7: [(seasonal_rolling_mean, 7, 8)] } ) backtest(ts) . Splits MSE: [0.71 0.91 0.86 0.54] Mean MSE: 0.75 . Date features . You can also specify date features to be computed, which are attributes of the ds column and are updated in each timestep as well. In this example the best model would be taking the average over each day of the week, which can be accomplished by doing one hot encoding on the day of the week column and fitting a linear model. . ts = TimeSeries(date_features=[&#39;dayofweek&#39;]) model = make_pipeline( OneHotEncoder(drop=&#39;first&#39;), LinearRegression(fit_intercept=False) ) backtest(ts, model) . Splits MSE: [0.6 0.61 0.95 0.5 ] Mean MSE: 0.67 . TL;DR . In this post we presented mlforecast, a framework that makes the use of machine learning models in forecasting tasks fast and easy. It allows you to focus on the model and features instead of implementation details. With mlforecast you can make experiemnts in an esasier way and it has a built-in backtesting functionality to help you find the best performing model. . Although this example contained only a single time series it is able to handle thousands of them and is very efficient both time and memory wise. . Next steps . mlforecast has more features like distributed training and a CLI. If you&#39;re interested you can learn more in the following resources: . GitHub repo: https://github.com/Nixtla/mlforecast | Documentation: https://nixtla.github.io/mlforecast/ | Example using mlforecast in the M5 competition: https://www.kaggle.com/lemuz90/m5-mlforecast | .",
            "url": "https://nixtla.github.io/blog/machine%20learning/forecasting/2021/06/10/Intro-mlforecast.html",
            "relUrl": "/machine%20learning/forecasting/2021/06/10/Intro-mlforecast.html",
            "date": " • Jun 10, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nixtla.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nixtla.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}