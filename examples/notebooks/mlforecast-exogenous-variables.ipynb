{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the required libraries for this article:\n",
    "\n",
    "```bash\n",
    "pip install mlforecast lightgbm utilsforecast\n",
    "```\n",
    "\n",
    "We'll use a subset of the [Kaggle Store Sales](https://www.kaggle.com/competitions/store-sales-time-series-forecasting) dataset. This dataset contains daily sales data from Corporación Favorita, a large Ecuadorian grocery retailer, with rich exogenous variables including store metadata, promotions, oil prices, and holidays.\n",
    "\n",
    "The subset contains 5 stores and 3 product families (GROCERY I, BEVERAGES, PRODUCE) from 2016-2017, merged with store metadata, oil prices, and holiday information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_URL = 'https://raw.githubusercontent.com/Nixtla/blog/refs/heads/main/examples/data/mlforecast_exogenous/store_sales_subset.csv'\n",
    "series = pd.read_csv(DATA_URL, parse_dates=['ds'])\n",
    "series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   | unique_id   | ds         | y      | store_nbr | family    | city  | state     | type | cluster | onpromotion | oil_price | is_holiday |\n",
    "|---|-------------|------------|--------|-----------|-----------|-------|-----------|------|---------|-------------|-----------|------------|\n",
    "| 0 | 1_BEVERAGES | 2016-01-01 | 0.0    | 1         | BEVERAGES | Quito | Pichincha | D    | 13      | 0           | 37.13     | 1          |\n",
    "| 1 | 1_BEVERAGES | 2016-01-02 | 1856.0 | 1         | BEVERAGES | Quito | Pichincha | D    | 13      | 7           | NaN       | 0          |\n",
    "| 2 | 1_BEVERAGES | 2016-01-03 | 1048.0 | 1         | BEVERAGES | Quito | Pichincha | D    | 13      | 1           | NaN       | 0          |\n",
    "| 3 | 1_BEVERAGES | 2016-01-04 | 3005.0 | 1         | BEVERAGES | Quito | Pichincha | D    | 13      | 3           | 36.81     | 0          |\n",
    "| 4 | 1_BEVERAGES | 2016-01-05 | 2374.0 | 1         | BEVERAGES | Quito | Pichincha | D    | 13      | 9           | 35.97     | 0          |\n",
    "\n",
    "\n",
    "The dataset contains 15 time series (5 stores × 3 product families) with:\n",
    "\n",
    "- **Static features**: `store_nbr`, `family`, `city`, `state`, `type`, `cluster`\n",
    "- **Dynamic features**: `onpromotion` (number of items on promotion), `oil_price` (daily oil price)\n",
    "- **Calendar feature**: `is_holiday` (whether the date is a holiday)\n",
    "\n",
    "Split the data into training and test sets. We'll hold out the last 7 days for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsforecast.losses import mae\n",
    "from utilsforecast.plotting import plot_series\n",
    "\n",
    "horizon = 7\n",
    "test = series.groupby('unique_id').tail(horizon).copy()\n",
    "train = series.drop(test.index).copy()\n",
    "print(f\"Train: {len(train)} rows, Test: {len(test)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Forecast\n",
    "\n",
    "Before adding exogenous variables, let's establish a baseline. This model uses only a single lag feature (yesterday's value), giving us a reference point to measure the impact of each exogenous variable type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from mlforecast import MLForecast\n",
    "\n",
    "fcst_baseline = MLForecast(\n",
    "    models=lgb.LGBMRegressor(n_jobs=1, random_state=0, verbosity=-1),\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    num_threads=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a complete list of parameters including `lags`, `date_features`, and `target_transforms`, see the [MLForecast API documentation](https://nixtlaverse.nixtla.io/mlforecast/forecast.html#mlforecast).\n",
    "\n",
    "Now fit the model to the training data. For the baseline, we use only the core time series columns without exogenous variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_baseline = train[['unique_id', 'ds', 'y']]\n",
    "fcst_baseline.fit(train_baseline)\n",
    "preds_baseline = fcst_baseline.predict(h=horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_baseline = test.merge(preds_baseline, on=['unique_id', 'ds'])\n",
    "baseline_mae = mae(eval_baseline, models=['LGBMRegressor'])['LGBMRegressor'].mean()\n",
    "print(f\"Baseline MAE: {baseline_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize how the baseline model performs without exogenous variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(\n",
    "    train,\n",
    "    forecasts_df=preds_baseline,\n",
    "    max_ids=4,\n",
    "    plot_random=False,\n",
    "    max_insample_length=50,\n",
    "    engine='matplotlib'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline forecasts flatten quickly after the first step. With only yesterday's value as input, the model can't anticipate the weekly spikes and dips visible in the historical data.\n",
    "\n",
    "## Static Features\n",
    "\n",
    "Static features represent time-invariant characteristics of each series. In retail forecasting, these might include store type, product category, or geographic region. Our dataset includes store metadata like `city`, `state`, `type`, and `cluster`.\n",
    "\n",
    "Without MLForecast, you would need to manually replicate these values when constructing features for prediction:\n",
    "\n",
    "```python\n",
    "# Manual approach: merge static features for prediction\n",
    "static_cols = ['store_nbr', 'family', 'city', 'state', 'type', 'cluster']\n",
    "static_df = series.groupby('unique_id')[static_cols].first().reset_index()\n",
    "future_dates = pd.DataFrame({'unique_id': ids, 'ds': future_timestamps})\n",
    "future_with_static = future_dates.merge(static_df, on='unique_id')\n",
    "```\n",
    "\n",
    "MLForecast handles this automatically. Specify static columns in the `static_features` parameter.\n",
    "\n",
    "First, convert string columns to categorical type so LightGBM can process them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string columns to categorical\n",
    "cat_cols = ['family', 'city', 'state', 'type']\n",
    "for col in cat_cols:\n",
    "    train[col] = train[col].astype('category')\n",
    "    test[col] = test[col].astype('category')\n",
    "\n",
    "static_cols = ['store_nbr', 'family', 'city', 'state', 'type', 'cluster']\n",
    "\n",
    "# Select only static features (exclude dynamic columns for now)\n",
    "train_static = train[['unique_id', 'ds', 'y'] + static_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model with static features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_static = MLForecast(\n",
    "    models=lgb.LGBMRegressor(n_jobs=1, random_state=0, verbosity=-1),\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    num_threads=2,\n",
    ")\n",
    "\n",
    "fcst_static.fit(train_static, static_features=static_cols)\n",
    "preds_static = fcst_static.predict(h=horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate and compare to baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_static = test.merge(preds_static, on=['unique_id', 'ds'])\n",
    "static_mae = mae(eval_static, models=['LGBMRegressor'])['LGBMRegressor'].mean()\n",
    "improvement = (baseline_mae - static_mae) / baseline_mae * 100\n",
    "print(f\"Static features MAE: {static_mae:.2f} ({improvement:.1f}% improvement over baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static features alone don't improve accuracy here because all stores in our subset are from Quito with similar characteristics. The model overfits to categorical noise rather than learning useful patterns.\n",
    "\n",
    "Visualize how static features affect the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(\n",
    "    train,\n",
    "    forecasts_df=preds_static,\n",
    "    max_ids=4,\n",
    "    plot_random=False,\n",
    "    max_insample_length=50,\n",
    "    engine='matplotlib'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flat predictions persist despite adding store metadata. With all stores from Quito sharing similar characteristics, the categorical features add noise rather than signal.\n",
    "\n",
    "## Dynamic Exogenous Variables\n",
    "\n",
    "Dynamic exogenous variables change over time but their future values are known in advance. Our dataset includes three dynamic features:\n",
    "\n",
    "- `onpromotion`: Number of items on promotion (retailers plan promotions ahead)\n",
    "- `oil_price`: Daily oil price (affects transportation costs and consumer spending in Ecuador)\n",
    "- `is_holiday`: Whether the date is a holiday (known from the calendar)\n",
    "\n",
    "The dataset already contains these features. We need to handle missing oil prices by forward-filling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for dynamic features (excluding is_holiday from training for now)\n",
    "dynamic_cols = ['onpromotion', 'oil_price', 'is_holiday']\n",
    "\n",
    "# Prepare training data with static and dynamic features\n",
    "train_dynamic = train[['unique_id', 'ds', 'y'] + static_cols + dynamic_cols].copy()\n",
    "train_dynamic['oil_price'] = train_dynamic['oil_price'].ffill()\n",
    "\n",
    "# Prepare future values for prediction\n",
    "future_dynamic = test[['unique_id', 'ds'] + dynamic_cols].copy()\n",
    "future_dynamic['oil_price'] = future_dynamic['oil_price'].ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with both static and dynamic features. The `onpromotion` and `oil_price` columns are automatically treated as dynamic since they're not listed in `static_features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_dynamic = MLForecast(\n",
    "    models=lgb.LGBMRegressor(n_jobs=1, random_state=0, verbosity=-1),\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    num_threads=2,\n",
    ")\n",
    "\n",
    "fcst_dynamic.fit(train_dynamic, static_features=static_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction, provide future values through the `X_df` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dynamic = fcst_dynamic.predict(h=horizon, X_df=future_dynamic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dynamic = test.merge(preds_dynamic, on=['unique_id', 'ds'])\n",
    "dynamic_mae = mae(eval_dynamic, models=['LGBMRegressor'])['LGBMRegressor'].mean()\n",
    "improvement = (baseline_mae - dynamic_mae) / baseline_mae * 100\n",
    "print(f\"Dynamic features MAE: {dynamic_mae:.2f} ({improvement:.1f}% improvement over baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic features MAE: 537.92 (22.5% improvement over baseline)\n",
    "\n",
    "Promotions and oil prices deliver significant gains. Products on promotion see predictable demand spikes, and oil prices affect transportation costs and consumer spending patterns in Ecuador's economy.\n",
    "\n",
    "Visualize how dynamic features improve the forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(\n",
    "    train,\n",
    "    forecasts_df=preds_dynamic,\n",
    "    max_ids=4,\n",
    "    plot_random=False,\n",
    "    max_insample_length=50,\n",
    "    engine='matplotlib'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forecasts now show variation instead of flat lines. Promotion counts and oil prices give the model actionable signals about when demand will shift.\n",
    "\n",
    "## Calendar Features\n",
    "\n",
    "Calendar patterns like day-of-week and month effects are common in time series data. Retail sales spike on weekends, energy consumption varies by season, and traffic patterns follow weekly cycles.\n",
    "\n",
    "MLForecast's `date_features` parameter extracts these patterns automatically. You can pass pandas datetime attributes or custom functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekend(dates):\n",
    "    return dates.dayofweek >= 5\n",
    "\n",
    "fcst_calendar = MLForecast(\n",
    "    models=lgb.LGBMRegressor(n_jobs=1, random_state=0, verbosity=-1),\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    date_features=['dayofweek', 'month', is_weekend],\n",
    "    num_threads=2,\n",
    ")\n",
    "\n",
    "fcst_calendar.fit(train_static, static_features=static_cols)\n",
    "preds_calendar = fcst_calendar.predict(h=horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common `date_features` options:\n",
    "\n",
    "- `dayofweek`: Day of week (0=Monday, 6=Sunday)\n",
    "- `month`: Month of year (1-12)\n",
    "- `dayofyear`: Day of year (1-366)\n",
    "- `quarter`: Quarter of year (1-4)\n",
    "- Custom functions like `is_weekend` above\n",
    "\n",
    "Evaluate and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_calendar = test.merge(preds_calendar, on=['unique_id', 'ds'])\n",
    "calendar_mae = mae(eval_calendar, models=['LGBMRegressor'])['LGBMRegressor'].mean()\n",
    "improvement = (baseline_mae - calendar_mae) / baseline_mae * 100\n",
    "print(f\"Calendar features MAE: {calendar_mae:.2f} ({improvement:.1f}% improvement over baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calendar features deliver the largest improvement at 44.5%. Grocery shopping follows strong weekly rhythms: customers stock up before weekends, avoid certain weekdays, and shift behavior around month boundaries. These patterns are consistent and directly encoded in the timestamp.\n",
    "\n",
    "Visualize how calendar features capture weekly patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(\n",
    "    train,\n",
    "    forecasts_df=preds_calendar,\n",
    "    max_ids=4,\n",
    "    plot_random=False,\n",
    "    max_insample_length=50,\n",
    "    engine='matplotlib'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forecasts now capture the weekly rhythm visible in the training data. Day-of-week features let the model distinguish high-traffic days from slower ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance with SHAP\n",
    "\n",
    "Understanding which features drive your forecasts helps validate model behavior and guide feature engineering. SHAP (SHapley Additive exPlanations) values show how each feature contributes to predictions across your dataset.\n",
    "\n",
    "First, extract the preprocessed features used during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = fcst_calendar.preprocess(train_static)\n",
    "X = prep.drop(columns=['unique_id', 'ds', 'y'])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute SHAP values using TreeExplainer, which is optimized for tree-based models like LightGBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(fcst_calendar.models_['LGBMRegressor'])\n",
    "shap_values = explainer(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize feature importance with a bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results validate our earlier findings:\n",
    "\n",
    "- `lag1` confirms yesterday's sales as the strongest predictor\n",
    "- `dayofweek` provides the most value among calendar features—consistent with the 44.5% accuracy improvement\n",
    "- Static features `city`, `state`, and `type` have zero impact, explaining why they didn't improve the baseline\n",
    "- `is_weekend` adds nothing despite `dayofweek` being important—the model already captures this pattern"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
